{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading channels sheet\n",
    "from os import listdir\n",
    "filepaths=[\"./channel/day=25/hour=17/\"+f for f in listdir(\"./channel/day=25/hour=17/\") if f.endswith('.csv')]\n",
    "rawchan=[]\n",
    "for filename in filepaths:\n",
    "    temp=pd.read_csv(filename, header=0, sep=\"|\")\n",
    "    rawchan.append(temp)\n",
    "df_channel=pd.concat(rawchan, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "b'Skipping line 294: expected 16 fields, saw 17\\n'\nb'Skipping line 18: expected 16 fields, saw 17\\n'\nb'Skipping line 191: expected 16 fields, saw 17\\n'\nb'Skipping line 86: expected 16 fields, saw 17\\n'\nb'Skipping line 206: expected 16 fields, saw 17\\n'\nb'Skipping line 23: expected 16 fields, saw 17\\n'\n"
    }
   ],
   "source": [
    "#loading video sheets\n",
    "rawvideo=[]\n",
    "filepaths=[\"./video/day=25/hour=17/\"+f for f in listdir(\"./video/day=25/hour=17/\") if f.endswith(\".csv\")]\n",
    "for filename in filepaths:\n",
    "    temp=pd.read_csv(filename, header=0, sep=\"|\", error_bad_lines=False)\n",
    "    rawvideo.append(temp)\n",
    "df_video=pd.concat(rawvideo, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove commentCount and rename channelId\n",
    "df_channel=df_channel.drop([\"commentCount\"], axis=1)\n",
    "df_channel=df_channel.rename(columns={\"id\":\"channelId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date and time conversion\n",
    "df_channel[\"collectedTime\"]=pd.to_datetime(df_channel.collectedTime)\n",
    "df_channel[\"publishedAt\"]=pd.to_datetime(df_channel.publishedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename videoId\n",
    "df_video=df_video.rename(columns={\"id\":\"videoId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date and time conversion\n",
    "df_video[\"collectedTime\"]=pd.to_datetime(df_video.collectedTime)\n",
    "df_video[\"publishedAt\"]=pd.to_datetime(df_video.publishedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title and category from channel to video\n",
    "chtovid=df_channel[[\"channelId\", \"title\", \"category\", \"sub_category\"]]#creating sub df#creating sub df\n",
    "chtovid=chtovid.rename(columns={\"channelId\":\"channelId\", \"title\":\"ch_title\", \"category\":\"ch_category\", \"sub_category\":\"ch_sub_category\"})\n",
    "df_video=pd.merge(df_video, chtovid, on=\"channelId\")#merge above title and category to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse date out of publishedAt from df_video\n",
    "df_video[\"pubDate\"]=df_video.publishedAt.astype(str).str[:-15]\n",
    "df_video[\"pubDate\"]=pd.to_datetime(df_video.pubDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse date out of publishedAt from df_channel\n",
    "df_channel[\"pubDate\"]=df_channel.publishedAt.astype(str).str[:-15]\n",
    "df_channel[\"pubDate\"]=pd.to_datetime(df_channel.pubDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse out collection date from df_channel\n",
    "df_channel[\"colDate\"]=df_channel.collectedTime.astype(str).str[:-15]\n",
    "df_channel[\"colDate\"]=pd.to_datetime(df_channel.colDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date since the channel was created\n",
    "df_channel[\"circa_D\"] = (df_channel.colDate - df_channel.pubDate).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest periodical data\n",
    "\n",
    "#get the last 1-day data\n",
    "df1D=df_video.set_index(\"pubDate\").sort_index().last(\"1D\")\n",
    "df1D=df1D.reset_index()\n",
    "#get the last 1-week data\n",
    "df1W=df_video.set_index(\"pubDate\").sort_index().last(\"1W\")\n",
    "df1W=df1W.reset_index()\n",
    "#get the last 1-month data\n",
    "df1M=df_video.set_index(\"pubDate\").sort_index().last(\"1M\")\n",
    "df1M=df1M.reset_index()\n",
    "#get the last 3-month data\n",
    "df3M=df_video.set_index(\"pubDate\").sort_index().last(\"3M\")\n",
    "df3M=df3M.reset_index()\n",
    "#get the last 6-month data\n",
    "df6M=df_video.set_index(\"pubDate\").sort_index().last(\"6M\")\n",
    "df6M=df6M.reset_index()\n",
    "#get the last 1-year data\n",
    "df1Y=df_video.set_index(\"pubDate\").sort_index().last(\"1Y\")\n",
    "df1Y=df1Y.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract numbers of vids per latest periods\n",
    "\n",
    "#1D\n",
    "vidcount=df1D.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_1D\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_1D\"]=df_channel[\"videoCount_1D\"].fillna(value=0)#set NAs into 0\n",
    "#1W\n",
    "vidcount=df1W.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_1W\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_1W\"]=df_channel[\"videoCount_1W\"].fillna(value=0)#set NAs into 0\n",
    "#1M\n",
    "vidcount=df1M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_1M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_1M\"]=df_channel[\"videoCount_1M\"].fillna(value=0)#set NAs into 0\n",
    "#3M\n",
    "vidcount=df3M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_3M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_3M\"]=df_channel[\"videoCount_3M\"].fillna(value=0)#set NAs into 0\n",
    "#6M\n",
    "vidcount=df6M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_6M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_6M\"]=df_channel[\"videoCount_6M\"].fillna(value=0)#set NAs into 0\n",
    "#1Y\n",
    "vidcount=df1Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_1Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_1Y\"]=df_channel[\"videoCount_1Y\"].fillna(value=0)#set NAs into 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract commentCount, like, dislike, view for latest periods\n",
    "#total\n",
    "like=df_video.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index()\n",
    "# like=like.drop([\"viewCount\", \"commentCount\"], axis=1).reset_index()#drop pointless columns and reset index\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#1D\n",
    "df1D=df1D.rename(columns={\"viewCount\":\"viewCount_1D\", \"likeCount\":\"likeCount_1D\", \"dislikeCount\":\"dislikeCount_1D\" ,\"commentCount\":\"commentCount_1D\"})\n",
    "like=df1D.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#1W\n",
    "df1W=df1W.rename(columns={\"viewCount\":\"viewCount_1W\", \"likeCount\":\"likeCount_1W\", \"dislikeCount\":\"dislikeCount_1W\", \"commentCount\":\"commentCount_1W\"})\n",
    "like=df1W.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#1M\n",
    "df1M=df1M.rename(columns={\"viewCount\":\"viewCount_1M\", \"likeCount\":\"likeCount_1M\", \"dislikeCount\":\"dislikeCount_1M\", \"commentCount\":\"commentCount_1M\"})\n",
    "like=df1M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#3M\n",
    "df3M=df3M.rename(columns={\"viewCount\":\"viewCount_3M\", \"likeCount\":\"likeCount_3M\", \"dislikeCount\":\"dislikeCount_3M\", \"commentCount\":\"commentCount_3M\"})\n",
    "like=df3M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#6M\n",
    "df6M=df6M.rename(columns={\"viewCount\":\"viewCount_6M\", \"likeCount\":\"likeCount_6M\", \"dislikeCount\":\"dislikeCount_6M\", \"commentCount\":\"commentCount_6M\"})\n",
    "like=df6M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#1Y\n",
    "df1Y=df1Y.rename(columns={\"viewCount\":\"viewCount_1Y\", \"likeCount\":\"likeCount_1Y\", \"dislikeCount\":\"dislikeCount_1Y\",\"commentCount\":\"commentCount_1Y\"})\n",
    "like=df1Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now calculating first periods' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first publication date for each channel from df_video\n",
    "firstvid=df_video.sort_values([\"channelId\", \"pubDate\"]).groupby([\"channelId\"]).first().reset_index()\n",
    "firstvid[\"dayone\"]=firstvid[\"pubDate\"]#avoiding naming conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    channelId     dayone        f1W        f1M        f3M  \\\n0    UC-8R1d-KQrZy_7-EvE_w9Nw 2019-10-30 2019-11-06 2019-11-30 2020-01-30   \n1    UC-9nTjw_qHCA693UA3jyDWw 2017-03-13 2017-03-20 2017-04-13 2017-06-13   \n2    UC-9oz6OofrZnaOUQ48-LavA 2014-10-15 2014-10-22 2014-11-15 2015-01-15   \n3    UC-AuHoBXFe2e6YQ3fcA2zFA 2015-04-23 2015-04-30 2015-05-23 2015-07-23   \n4    UC-BqPABOl7c7rR5EoJJZ0UA 2016-10-11 2016-10-18 2016-11-11 2017-01-11   \n..                        ...        ...        ...        ...        ...   \n976  UCzXNBo1PYNsqSrN19XK3hPw 2017-10-09 2017-10-16 2017-11-09 2018-01-09   \n977  UCzacSE_ExLj6L5rXx2CakjQ 2016-07-04 2016-07-11 2016-08-04 2016-10-04   \n978  UCzdPXIkTEXBkbx9cQ9aYDJA 2016-01-17 2016-01-24 2016-02-17 2016-04-17   \n979  UCzmGNr7bHavW8SUg1n4KIMQ 2016-02-24 2016-03-02 2016-03-24 2016-05-24   \n980  UCzte3fAK5m-9KswRH3SPNuA 2016-05-03 2016-05-10 2016-06-03 2016-08-03   \n\n           f6M        f1Y        f2Y        f3Y        f4Y        f5Y  \n0   2020-04-30 2020-10-30 2021-10-30 2022-10-30 2023-10-30 2024-10-30  \n1   2017-09-13 2018-03-13 2019-03-13 2020-03-13 2021-03-13 2022-03-13  \n2   2015-04-15 2015-10-15 2016-10-15 2017-10-15 2018-10-15 2019-10-15  \n3   2015-10-23 2016-04-23 2017-04-23 2018-04-23 2019-04-23 2020-04-23  \n4   2017-04-11 2017-10-11 2018-10-11 2019-10-11 2020-10-11 2021-10-11  \n..         ...        ...        ...        ...        ...        ...  \n976 2018-04-09 2018-10-09 2019-10-09 2020-10-09 2021-10-09 2022-10-09  \n977 2017-01-04 2017-07-04 2018-07-04 2019-07-04 2020-07-04 2021-07-04  \n978 2016-07-17 2017-01-17 2018-01-17 2019-01-17 2020-01-17 2021-01-17  \n979 2016-08-24 2017-02-24 2018-02-24 2019-02-24 2020-02-24 2021-02-24  \n980 2016-11-03 2017-05-03 2018-05-03 2019-05-03 2020-05-03 2021-05-03  \n\n[981 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>channelId</th>\n      <th>dayone</th>\n      <th>f1W</th>\n      <th>f1M</th>\n      <th>f3M</th>\n      <th>f6M</th>\n      <th>f1Y</th>\n      <th>f2Y</th>\n      <th>f3Y</th>\n      <th>f4Y</th>\n      <th>f5Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UC-8R1d-KQrZy_7-EvE_w9Nw</td>\n      <td>2019-10-30</td>\n      <td>2019-11-06</td>\n      <td>2019-11-30</td>\n      <td>2020-01-30</td>\n      <td>2020-04-30</td>\n      <td>2020-10-30</td>\n      <td>2021-10-30</td>\n      <td>2022-10-30</td>\n      <td>2023-10-30</td>\n      <td>2024-10-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UC-9nTjw_qHCA693UA3jyDWw</td>\n      <td>2017-03-13</td>\n      <td>2017-03-20</td>\n      <td>2017-04-13</td>\n      <td>2017-06-13</td>\n      <td>2017-09-13</td>\n      <td>2018-03-13</td>\n      <td>2019-03-13</td>\n      <td>2020-03-13</td>\n      <td>2021-03-13</td>\n      <td>2022-03-13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UC-9oz6OofrZnaOUQ48-LavA</td>\n      <td>2014-10-15</td>\n      <td>2014-10-22</td>\n      <td>2014-11-15</td>\n      <td>2015-01-15</td>\n      <td>2015-04-15</td>\n      <td>2015-10-15</td>\n      <td>2016-10-15</td>\n      <td>2017-10-15</td>\n      <td>2018-10-15</td>\n      <td>2019-10-15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UC-AuHoBXFe2e6YQ3fcA2zFA</td>\n      <td>2015-04-23</td>\n      <td>2015-04-30</td>\n      <td>2015-05-23</td>\n      <td>2015-07-23</td>\n      <td>2015-10-23</td>\n      <td>2016-04-23</td>\n      <td>2017-04-23</td>\n      <td>2018-04-23</td>\n      <td>2019-04-23</td>\n      <td>2020-04-23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UC-BqPABOl7c7rR5EoJJZ0UA</td>\n      <td>2016-10-11</td>\n      <td>2016-10-18</td>\n      <td>2016-11-11</td>\n      <td>2017-01-11</td>\n      <td>2017-04-11</td>\n      <td>2017-10-11</td>\n      <td>2018-10-11</td>\n      <td>2019-10-11</td>\n      <td>2020-10-11</td>\n      <td>2021-10-11</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>976</th>\n      <td>UCzXNBo1PYNsqSrN19XK3hPw</td>\n      <td>2017-10-09</td>\n      <td>2017-10-16</td>\n      <td>2017-11-09</td>\n      <td>2018-01-09</td>\n      <td>2018-04-09</td>\n      <td>2018-10-09</td>\n      <td>2019-10-09</td>\n      <td>2020-10-09</td>\n      <td>2021-10-09</td>\n      <td>2022-10-09</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>UCzacSE_ExLj6L5rXx2CakjQ</td>\n      <td>2016-07-04</td>\n      <td>2016-07-11</td>\n      <td>2016-08-04</td>\n      <td>2016-10-04</td>\n      <td>2017-01-04</td>\n      <td>2017-07-04</td>\n      <td>2018-07-04</td>\n      <td>2019-07-04</td>\n      <td>2020-07-04</td>\n      <td>2021-07-04</td>\n    </tr>\n    <tr>\n      <th>978</th>\n      <td>UCzdPXIkTEXBkbx9cQ9aYDJA</td>\n      <td>2016-01-17</td>\n      <td>2016-01-24</td>\n      <td>2016-02-17</td>\n      <td>2016-04-17</td>\n      <td>2016-07-17</td>\n      <td>2017-01-17</td>\n      <td>2018-01-17</td>\n      <td>2019-01-17</td>\n      <td>2020-01-17</td>\n      <td>2021-01-17</td>\n    </tr>\n    <tr>\n      <th>979</th>\n      <td>UCzmGNr7bHavW8SUg1n4KIMQ</td>\n      <td>2016-02-24</td>\n      <td>2016-03-02</td>\n      <td>2016-03-24</td>\n      <td>2016-05-24</td>\n      <td>2016-08-24</td>\n      <td>2017-02-24</td>\n      <td>2018-02-24</td>\n      <td>2019-02-24</td>\n      <td>2020-02-24</td>\n      <td>2021-02-24</td>\n    </tr>\n    <tr>\n      <th>980</th>\n      <td>UCzte3fAK5m-9KswRH3SPNuA</td>\n      <td>2016-05-03</td>\n      <td>2016-05-10</td>\n      <td>2016-06-03</td>\n      <td>2016-08-03</td>\n      <td>2016-11-03</td>\n      <td>2017-05-03</td>\n      <td>2018-05-03</td>\n      <td>2019-05-03</td>\n      <td>2020-05-03</td>\n      <td>2021-05-03</td>\n    </tr>\n  </tbody>\n</table>\n<p>981 rows × 11 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#change firstvid into viddate and add term periods\n",
    "viddate=firstvid[[\"channelId\", \"dayone\"]]\n",
    "viddate\n",
    "viddate[\"f1W\"]=viddate.dayone+pd.DateOffset(weeks=1)\n",
    "viddate[\"f1M\"]=viddate.dayone+pd.DateOffset(months=1)\n",
    "viddate[\"f3M\"]=viddate.dayone+pd.DateOffset(months=3)\n",
    "viddate[\"f6M\"]=viddate.dayone+pd.DateOffset(months=6)\n",
    "viddate[\"f1Y\"]=viddate.dayone+pd.DateOffset(years=1)\n",
    "viddate[\"f2Y\"]=viddate.dayone+pd.DateOffset(years=2)\n",
    "viddate[\"f3Y\"]=viddate.dayone+pd.DateOffset(years=3)\n",
    "viddate[\"f4Y\"]=viddate.dayone+pd.DateOffset(years=4)\n",
    "viddate[\"f5Y\"]=viddate.dayone+pd.DateOffset(years=5)\n",
    "viddate#created term periods for each channel circa their first pubDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add term periods to df_video\n",
    "df_video=pd.merge(df_video, viddate, on=\"channelId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dfs with each time period\n",
    "df_f1W=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f1W)]\n",
    "df_f1M=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f1M)]\n",
    "df_f3M=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f3M)]\n",
    "df_f6M=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f6M)]\n",
    "df_f1Y=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f1Y)]\n",
    "df_f2Y=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f2Y)]\n",
    "df_f3Y=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f3Y)]\n",
    "df_f4Y=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f4Y)]\n",
    "df_f5Y=df_video[(df_video.pubDate>=df_video.dayone) & (df_video.pubDate<=df_video.f5Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract numbers of vids for first periods\n",
    "\n",
    "#f1W\n",
    "vidcount=df_f1W.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f1W\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f1W\"]=df_channel[\"videoCount_f1W\"].fillna(value=0)#set NAs into 0\n",
    "#f1M\n",
    "vidcount=df_f1M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f1M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f1M\"]=df_channel[\"videoCount_f1M\"].fillna(value=0)#set NAs into 0\n",
    "#f3M\n",
    "vidcount=df_f3M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f3M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f3M\"]=df_channel[\"videoCount_f3M\"].fillna(value=0)#set NAs into 0\n",
    "#f6M\n",
    "vidcount=df_f6M.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f6M\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f6M\"]=df_channel[\"videoCount_f6M\"].fillna(value=0)#set NAs into 0\n",
    "#f1Y\n",
    "vidcount=df_f1Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f1Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f1Y\"]=df_channel[\"videoCount_f1Y\"].fillna(value=0)#set NAs into 0\n",
    "#f2Y\n",
    "vidcount=df_f2Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f2Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f2Y\"]=df_channel[\"videoCount_f2Y\"].fillna(value=0)#set NAs into 0\n",
    "#f3Y\n",
    "vidcount=df_f3Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f3Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f3Y\"]=df_channel[\"videoCount_f3Y\"].fillna(value=0)#set NAs into 0\n",
    "#f4Y\n",
    "vidcount=df_f4Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f4Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f4Y\"]=df_channel[\"videoCount_f4Y\"].fillna(value=0)#set NAs into 0\n",
    "#f5Y\n",
    "vidcount=df_f5Y.channelId.value_counts().rename_axis(\"channelId\").reset_index(name=\"videoCount_f5Y\")#video counts for the period in new df\n",
    "df_channel=pd.merge(df_channel, vidcount, on=\"channelId\", how=\"left\")#merge into df_channel\n",
    "df_channel[\"videoCount_f5Y\"]=df_channel[\"videoCount_f5Y\"].fillna(value=0)#set NAs into 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract commentCount, like, dislike, view for latest periods\n",
    "\n",
    "#f1W\n",
    "df_f1W=df_f1W.rename(columns={\"viewCount\":\"viewCount_f1W\", \"likeCount\":\"likeCount_f1W\", \"dislikeCount\":\"dislikeCount_f1W\" ,\"commentCount\":\"commentCount_f1W\"})\n",
    "like=df_f1W.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f1M\n",
    "df_f1M=df_f1M.rename(columns={\"viewCount\":\"viewCount_f1M\", \"likeCount\":\"likeCount_f1M\", \"dislikeCount\":\"dislikeCount_f1M\" ,\"commentCount\":\"commentCount_f1M\"})\n",
    "like=df_f1M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f3M\n",
    "df_f3M=df_f3M.rename(columns={\"viewCount\":\"viewCount_f3M\", \"likeCount\":\"likeCount_f3M\", \"dislikeCount\":\"dislikeCount_f3M\" ,\"commentCount\":\"commentCount_f3M\"})\n",
    "like=df_f3M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f6M\n",
    "df_f6M=df_f6M.rename(columns={\"viewCount\":\"viewCount_f6M\", \"likeCount\":\"likeCount_f6M\", \"dislikeCount\":\"dislikeCount_f6M\" ,\"commentCount\":\"commentCount_f6M\"})\n",
    "like=df_f6M.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f1Y\n",
    "df_f1Y=df_f1Y.rename(columns={\"viewCount\":\"viewCount_f1Y\", \"likeCount\":\"likeCount_f1Y\", \"dislikeCount\":\"dislikeCount_f1Y\" ,\"commentCount\":\"commentCount_f1Y\"})\n",
    "like=df_f1Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f2Y\n",
    "df_f2Y=df_f2Y.rename(columns={\"viewCount\":\"viewCount_f2Y\", \"likeCount\":\"likeCount_f2Y\", \"dislikeCount\":\"dislikeCount_f2Y\" ,\"commentCount\":\"commentCount_f2Y\"})\n",
    "like=df_f2Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f3Y\n",
    "df_f3Y=df_f3Y.rename(columns={\"viewCount\":\"viewCount_f3Y\", \"likeCount\":\"likeCount_f3Y\", \"dislikeCount\":\"dislikeCount_f3Y\" ,\"commentCount\":\"commentCount_f3Y\"})\n",
    "like=df_f3Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f4Y\n",
    "df_f4Y=df_f4Y.rename(columns={\"viewCount\":\"viewCount_f4Y\", \"likeCount\":\"likeCount_f4Y\", \"dislikeCount\":\"dislikeCount_f4Y\" ,\"commentCount\":\"commentCount_f4Y\"})\n",
    "like=df_f4Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n",
    "#f5Y\n",
    "df_f5Y=df_f5Y.rename(columns={\"viewCount\":\"viewCount_f5Y\", \"likeCount\":\"likeCount_f5Y\", \"dislikeCount\":\"dislikeCount_f5Y\" ,\"commentCount\":\"commentCount_f5Y\"})\n",
    "like=df_f5Y.groupby([\"channelId\"]).sum()#get sum\n",
    "like=like.reset_index().fillna(value=0)\n",
    "df_channel=pd.merge(df_channel, like, on=\"channelId\", how=\"left\")#merge like and channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channel.to_csv(\"AllData.csv\")"
   ]
  }
 ]
}